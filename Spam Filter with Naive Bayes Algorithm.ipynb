{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-dispatch",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project I will be building a simple (but hopefully effective) spam filter for E-Mails using a naive Bayes approach on the Enron-Spam dataset. The dataset contains 33.716 Emails marked as spam or as non-spam (\"ham\") messages. The original dataset was collected by V. Metsis, I. Androutsopoulos and G. Paliouras. and can be found [here](http://www2.aueb.gr/users/ion/data/enron-spam). I packaged all the data into a [single csv-file](https://github.com/MWiechmann/enron_spam_data), which I will be using for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-visitor",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abandoned-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import csv\n",
    "\n",
    "mails = pd.read_csv(\"enron_spam_data.zip\", compression= \"zip\", index_col=\"Message ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "grand-comment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vastar resources , inc .</td>\n",
       "      <td>gary , production from the high island larger ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calpine daily gas nomination</td>\n",
       "      <td>- calpine daily gas nomination 1 . doc</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>re : issue</td>\n",
       "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meter 7268 nov allocation</td>\n",
       "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
       "      <td>ham</td>\n",
       "      <td>1999-12-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Subject  \\\n",
       "Message ID                                 \n",
       "0           christmas tree farm pictures   \n",
       "1               vastar resources , inc .   \n",
       "2           calpine daily gas nomination   \n",
       "3                             re : issue   \n",
       "4              meter 7268 nov allocation   \n",
       "\n",
       "                                                      Message Spam/Ham  \\\n",
       "Message ID                                                               \n",
       "0                                                         NaN      ham   \n",
       "1           gary , production from the high island larger ...      ham   \n",
       "2                      - calpine daily gas nomination 1 . doc      ham   \n",
       "3           fyi - see note below - already done .\\nstella\\...      ham   \n",
       "4           fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham   \n",
       "\n",
       "                  Date  \n",
       "Message ID              \n",
       "0           1999-12-10  \n",
       "1           1999-12-13  \n",
       "2           1999-12-14  \n",
       "3           1999-12-14  \n",
       "4           1999-12-14  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTotal:\\t\" + str(mails.shape[0]))\n",
    "print(mails[\"Spam/Ham\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nProportion in %\")\n",
    "print(round(mails[\"Spam/Ham\"].value_counts(normalize=True), 4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-console",
   "metadata": {},
   "source": [
    "## Setting up Training & Testing Data Set\n",
    "I will be using 80% of the data as training set, and the remaining 20% as testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize dataset\n",
    "mails = mails.sample(frac=1, random_state=42)\n",
    "\n",
    "# Reindex after randomization\n",
    "mails.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Get 80% as training data, rest as test data\n",
    "cutoff_index = int(round(mails.shape[0] * 0.8, 0))\n",
    "\n",
    "train = mails.iloc[:cutoff_index].copy(deep=True)\n",
    "test = mails.iloc[cutoff_index:].copy(deep=True)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAINING DATA:\")\n",
    "print(\"Proportion in %\")\n",
    "print(round(train[\"Spam/Ham\"].value_counts(normalize=True), 4)*100)\n",
    "\n",
    "print(\"\\nTESTING DATA:\")\n",
    "print(\"Proportion in %\")\n",
    "print(round(test[\"Spam/Ham\"].value_counts(normalize=True), 4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-chassis",
   "metadata": {},
   "source": [
    "Proportions are comparable in both datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-painting",
   "metadata": {},
   "source": [
    "# Prepping Data for Naive Bayes\n",
    "In this step I will prepare the data for both the subject as well as the data for the actual email message for later processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-threat",
   "metadata": {},
   "source": [
    "## Cleaning the Message Word String for Subject Line and Email Body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-civilization",
   "metadata": {},
   "source": [
    "### Prepping Subject Line Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation and convert everything to lowercase\n",
    "train[\"Subject\"] = train[\"Subject\"].str.replace(\n",
    "    \"\\W\", \" \", regex=True).str.lower()\n",
    "# The method above is quick but occasionally produces double spaces - cleaning this up just in case\n",
    "train[\"Subject\"] = train[\"Subject\"].str.replace(\n",
    "    \"\\s{2,}\", \" \", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Subject\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-necklace",
   "metadata": {},
   "source": [
    "### Prepping Message Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation and convert everything to lowercase\n",
    "train[\"Message\"] = train[\"Message\"].str.replace(\n",
    "    \"\\W\", \" \", regex=True).str.lower()\n",
    "# The method above is quick but produces double spaces - cleaning this up\n",
    "train[\"Message\"] = train[\"Message\"].str.replace(\n",
    "    \"\\s{2,}\", \" \", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Message\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-dictionary",
   "metadata": {},
   "source": [
    "## Building the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-harmony",
   "metadata": {},
   "source": [
    "### Vocabulary: Subject Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Subject line to list\n",
    "train[\"Subject\"] = train[\"Subject\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "subject_voc = []\n",
    "# Add each single word of each message to the vocabulary\n",
    "for index, subject in train[\"Subject\"].iteritems():\n",
    "    if type(subject) == list:\n",
    "        # ignore instance with blank subject lines where the split resulted in nan object instead of list\n",
    "        for word in subject:\n",
    "            subject_voc.append(word)\n",
    "# Get rid of duplicate words\n",
    "subject_voc = list(set(subject_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique words in subject line of training data set:\")\n",
    "print(len(subject_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dictonary with word count for each subject line\n",
    "\n",
    "# Create a dictionary with all unique words as keys and a list as entry\n",
    "# the list contains one count for each subject line (row) - will be filled in the next step\n",
    "# because of reindexing ID simply starts at 0...\n",
    "word_counts_per_subject[\"Message ID\"] = list(range(train.shape[0]))\n",
    "word_counts_per_subject = {word: [0]*train.shape[0] for word in subject_voc}\n",
    "\n",
    "\n",
    "# loop over all the subject lines for each contained word\n",
    "# increase the count in appropriate place in the dictionary by +1\n",
    "for index, subject in train[\"Subject\"].iteritems():\n",
    "    for word in subject:\n",
    "        word_counts_per_subject[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to csv file\n",
    "\n",
    "'''\n",
    "The data can still be handled by pandas but is getting a bit large to be handled comfortably on all machines (1GB+)\n",
    "This is of course more of a problem for the latter analysis of email message data which will usually not fit into memory.\n",
    "\n",
    "Therefore the data will be exported to a csv file so that it can later be read in to a dask dataframe.\n",
    "\n",
    "'''\n",
    "\n",
    "def save_dict_to_csv(dictionary, output_file_name, progress_step = 5):\n",
    "    print(\"...opening/creting file\" + output_file_name)\n",
    "    with open(output_file_name, 'w', newline='') as csvfile:\n",
    "        print(\"...writing dictonary to file...\")\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(dictionary)  # First row (the keys of the dictionary).\n",
    "        print(\"...wrote header to file...\")\n",
    "        print(\"...now writing values to file...\")\n",
    "        \n",
    "        dict_vals_zip = zip(*dictionary.values())\n",
    "        \n",
    "        # vals for progress messages\n",
    "        steps = len(list(dict_vals_zip))\n",
    "        steps_mult = round(steps / 100)\n",
    "        current_step = 0\n",
    "        progress = 0\n",
    "        \n",
    "        for values in zip(*dictionary.values()):\n",
    "            writer.writerow(values)\n",
    "            \n",
    "            current_step += 1\n",
    "            \n",
    "            if current_step%steps_mult == 0:\n",
    "                progress += 1\n",
    "                if progress%progress_step == 0:\n",
    "                    print(\"...about \" + str(progress) + \"% done...\")\n",
    "    \n",
    "    print(\"...DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(word_counts_per_subject, 'test_output3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-active",
   "metadata": {},
   "source": [
    "### Vocabulary: Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Messages to list\n",
    "train[\"Message\"] = train[\"Message\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the message_vocabulary\n",
    "message_voc = []\n",
    "# Add each single word of each message to the message_vocabulary\n",
    "for index, message in train[\"Message\"].iteritems():\n",
    "    if type(message) == list:\n",
    "        # ignore instance with blank message where the split resulted in nan object instead of list\n",
    "        for word in message:\n",
    "            message_voc.append(word)\n",
    "# Get rid of duplicate words\n",
    "message_voc = list(set(message_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique words in email message of training data set:\")\n",
    "print(len(message_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataframe with word count for each message\n",
    "mask_spam = train[\"Spam/Ham\"] == \"spam\"\n",
    "mask_ham = train[\"Spam/Ham\"] == \"ham\"\n",
    "\n",
    "train_spam = train[mask_spam]\n",
    "train_ham = train[mask_ham]\n",
    "\n",
    "# Create a dictionary with all unique words as keys and a list as entry\n",
    "# the list contains one count for each email message (row) - will be filled in the next step\n",
    "# word_counts_per_message = {word: [0]*train.shape[0] for word in message_voc}\n",
    "\n",
    "\n",
    "\n",
    "# loop over all the messages for each contained word\n",
    "# increase the count in appropriate place in the dictionary by +1\n",
    "# for index, subject in train[\"Message\"].iteritems():\n",
    "#     for word in message:\n",
    "#         word_counts_per_message[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-steam",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 186.4,
   "position": {
    "height": "320.4px",
    "left": "763px",
    "right": "20px",
    "top": "11px",
    "width": "471px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
