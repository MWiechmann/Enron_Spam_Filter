{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-dispatch",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project I will be building a simple (but hopefully effective) spam filter for E-Mails using a naive Bayes approach on the Enron-Spam dataset. The dataset contains 33.716 Emails marked as spam or as non-spam (\"ham\") messages. The original dataset was collected by V. Metsis, I. Androutsopoulos and G. Paliouras. and can be found [here](http://www2.aueb.gr/users/ion/data/enron-spam). I packaged all the data into a [single zipped csv-file](https://github.com/MWiechmann/enron_spam_data), which I will be using for this project.\n",
    "\n",
    "I plan to first try out an algorithm based on the subject line. Processing the subject line should be considerably quicker than a solution based on the email body. If this solution does not perform satisfactory (accuracy or precision < 10%) I will combine this with processing the body of the email for edge cases.\n",
    "\n",
    "Beforehand, the data-file was processed with the python script \"process_data.py\"\n",
    "This results in the following data in the /data folder:\n",
    "\n",
    "File | Content\n",
    "- | -\n",
    "enron_spam_data.zip | The raw Enron-Spam data set from my repo [here](https://github.com/MWiechmann/enron_spam_data). A zipped csv-file that contains the columns Subject (subject line), Message (email body), Spam/Ham (email category encoded \"ham\" or \"spam\") and Date (date of the email in the format YYYY-MM-DD)\n",
    "train.zip | 80% of the original data set for training the model. The structure is similar to the the Enron-Spam data (above), but Subject and Message have been preprocessed: The string has been converted to lowercase, punctuation has been removed. Additionally, the string has been converted to a list with one value per word.\n",
    "test.zip | 20% of the original data set for testing the model. No further preprocessing has been done on this data.\n",
    "subject_voc.zip | Zipped csv-file that contains the word count of all unique words from the subject line (based on the training data). Each unique word is represented by a column and each subject line of the train is represented by a single line. So each line contains the word count per word for this subject line.\n",
    "message_voc_zip | Zipped csv file with the vocabulary for the email text bodies. Structure is identical to subject_voc.zip (above) so each line contains the word count per word for a single email message body."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-visitor",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abandoned-executive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['re', 'tenaska', 'iv']</td>\n",
       "      <td>['i', 'tried', 'calling', 'you', 'this', 'am',...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['neon']</td>\n",
       "      <td>['bammel', 'neon', 'groups', 'fall', '2001', '...</td>\n",
       "      <td>ham</td>\n",
       "      <td>2001-09-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['fw', 're', 'ivanhoe', 'e', 's', 'd']</td>\n",
       "      <td>['fyi', 'kim', 'original', 'message', 'from', ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['start', 'date', '2', '6', '02', 'hourahead',...</td>\n",
       "      <td>['start', 'date', '2', '6', '02', 'hourahead',...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['fw', 're', 'ivanhoe', 'e', 's', 'd']</td>\n",
       "      <td>['fyi', 'kim', 'original', 'message', 'from', ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-09-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Subject  \\\n",
       "Message ID                                                      \n",
       "0                                     ['re', 'tenaska', 'iv']   \n",
       "1                                                    ['neon']   \n",
       "2                      ['fw', 're', 'ivanhoe', 'e', 's', 'd']   \n",
       "3           ['start', 'date', '2', '6', '02', 'hourahead',...   \n",
       "4                      ['fw', 're', 'ivanhoe', 'e', 's', 'd']   \n",
       "\n",
       "                                                      Message Spam/Ham  \\\n",
       "Message ID                                                               \n",
       "0           ['i', 'tried', 'calling', 'you', 'this', 'am',...     spam   \n",
       "1           ['bammel', 'neon', 'groups', 'fall', '2001', '...      ham   \n",
       "2           ['fyi', 'kim', 'original', 'message', 'from', ...     spam   \n",
       "3           ['start', 'date', '2', '6', '02', 'hourahead',...     spam   \n",
       "4           ['fyi', 'kim', 'original', 'message', 'from', ...     spam   \n",
       "\n",
       "                  Date  \n",
       "Message ID              \n",
       "0           2004-02-25  \n",
       "1           2001-09-24  \n",
       "2           2004-12-17  \n",
       "3           2005-08-31  \n",
       "4           2004-09-07  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"data/train_data.zip\",\n",
    "                    compression=\"zip\", index_col=\"Message ID\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worth-blood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count\n",
      "spam    13716\n",
      "ham     13257\n",
      "Name: Spam/Ham, dtype: int64\n",
      "\n",
      "Proportion in %\n",
      "spam    50.85\n",
      "ham     49.15\n",
      "Name: Spam/Ham, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Count\")\n",
    "print(train[\"Spam/Ham\"].value_counts(dropna=False))\n",
    "print(\"\\nProportion in %\")\n",
    "print(round(train[\"Spam/Ham\"].value_counts(normalize=True), 4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "announced-checkout",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in subject vocabulary dataframe with dask \n",
    "# (machines with a lot of Ram will be fine reading this to a pandas Dataframe, but want to be safe here)\n",
    "\n",
    "if not os.path.exists(\"data/subject_voc.csv\"):\n",
    "    shutil.unpack_archive(\"data/subject_voc.csv\", \"data/\")\n",
    "subject_voc = dd.read_csv(\"data/subject_voc.csv\").set_index(\"Message ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-highland",
   "metadata": {},
   "source": [
    "# Building the Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-ecology",
   "metadata": {},
   "source": [
    "## General Constant Parameters for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "automotive-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability for Spam and Non-Spam (Ham)\n",
    "p_spam = train[\"Spam/Ham\"].value_counts(normalize=True)[\"spam\"]\n",
    "p_ham = train[\"Spam/Ham\"].value_counts(normalize=True)[\"ham\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-break",
   "metadata": {},
   "source": [
    "## Spam Filter based on Subject Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-paper",
   "metadata": {},
   "source": [
    "### Calculating Subject Line Specific Constant Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "integral-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Counts for Spam/ham\n",
    "train_spam = train[train[\"Spam/Ham\"] == \"spam\"]\n",
    "train_ham = train[train[\"Spam/Ham\"] == \"ham\"]\n",
    "\n",
    "n_words_subject_spam = train_spam[\"Subject\"].apply(len).sum()\n",
    "n_words_subject_ham = train_ham[\"Subject\"].apply(len).sum()\n",
    "# (could also use train[mask_spam].iloc[:,2:].sum().sum() above, but takes approx 2x as long)\n",
    "\n",
    "# Unique word count for vocabulary\n",
    "n_words_subject_voc = subject_voc.shape[1]\n",
    "\n",
    "# Smoothin parameter\n",
    "alpha = 1\n",
    "\n",
    "# Delete train_spam and train_ham again to save memory\n",
    "del train_spam, train_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-vienna",
   "metadata": {},
   "source": [
    "### Calculate Word-Specific Parameters (Subject Line)\n",
    "Computing the word-specific probabilities takes a bit of time. Therefore they will only be computed if they have not been saved to file yet. Otherwise, the probabilities will just be read from the csv-files. If you want to redo the computation of probabilities, you need to delete the files 'data/p_subject_word_given_spam.csv' and 'data/p_subject_word_given_ham.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broke-restriction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files with probabilities for subject line words found. Reading in probabilities to dictonaries......Done!\n"
     ]
    }
   ],
   "source": [
    "# only compute word-specific probabilities if not already saved to file\n",
    "if not (os.path.exists(\"data/p_subject_word_given_spam.csv\") and os.path.exists(\"data/p_subject_word_given_ham.csv\")):\n",
    "    '''\n",
    "    Doing the following computation from dask dataframes is extremly slow.\n",
    "    Instead I will create a series of pandas dataframes from the main dask dataframe to perform the operation on\n",
    "    the result will be stored on file (just in case).\n",
    "    For this dataset, pandas dataframes with 2500 columns seem to work without memory problems on my machine\n",
    "    Therefore the following process will go through the bigger dask dataframe with smaller pandas dataframe, \n",
    "    each containing 2500 columns.\n",
    "    '''\n",
    "\n",
    "    print(\"No files with probabilities for subject line words found. Creating dict with probabilities given spam/ham for \" +\n",
    "          str(n_words_subject_voc) + \" words...\")\n",
    "\n",
    "    # Build dictionaries with word-specific probability given either spam or non-spam (ham)\n",
    "    p_subject_word_given_spam_dict = {word: 0.0 for word in subject_voc.columns}\n",
    "    p_subject_word_given_ham_dict = {word: 0.0 for word in subject_voc.columns}\n",
    "\n",
    "    # Determine slice endpoints to go through dask dataframe in 2500 word steps\n",
    "    endpoints = list(range(2500, n_words_subject_voc, 2500))\n",
    "    endpoints.append(n_words_subject_voc)\n",
    "\n",
    "    step = 1\n",
    "\n",
    "    for endpoint in endpoints:\n",
    "        print(\"...creating dictonary - step \" + str(step) + \"/\" +\n",
    "              str(len(endpoints)) + \"...\", end=\"\\r\")\n",
    "\n",
    "        subject_voc_words_step = list(subject_voc.columns[:endpoint])\n",
    "\n",
    "        # Limit subject vocabulary dataframe to the 2500 words in this step, \n",
    "        # Add Spam/Ham to dataframe\n",
    "        # Seperate dataframe into spam/ham dataframes\n",
    "        # Then transform from dask to pandas dataframe\n",
    "        subject_voc_spam = subject_voc[subject_voc_words_step].copy()\n",
    "        subject_voc_ham = subject_voc[subject_voc_words_step].copy()\n",
    "        subject_voc_spam[\"Spam/Ham\"] = train[\"Spam/Ham\"]\n",
    "        subject_voc_ham[\"Spam/Ham\"] = train[\"Spam/Ham\"]\n",
    "        subject_voc_spam = subject_voc_spam[subject_voc_ham[\"Spam/Ham\"] == \"spam\"]\n",
    "        subject_voc_ham = subject_voc_ham[subject_voc_ham[\"Spam/Ham\"] == \"ham\"]\n",
    "        subject_voc_spam = subject_voc_spam.compute()\n",
    "        subject_voc_ham = subject_voc_ham.compute()\n",
    "\n",
    "        for word in subject_voc_words_step:\n",
    "            n_word_given_spam = subject_voc_spam[word].sum()\n",
    "            prob = (n_word_given_spam + alpha) / \\\n",
    "                (n_words_subject_spam + alpha * n_words_subject_voc)\n",
    "            p_subject_word_given_spam_dict[word] = prob\n",
    "\n",
    "            n_word_given_ham = subject_voc_ham[word].sum()\n",
    "            prob = (n_word_given_ham + alpha) / \\\n",
    "                (n_words_subject_ham + alpha * n_words_subject_voc)\n",
    "            p_subject_word_given_ham_dict[word] = prob\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    print(\"...dictonary created!                     \")\n",
    "\n",
    "    print(\"Now saving dictonaries with probabilities to file...\")\n",
    "\n",
    "    with open(\"data/p_subject_word_given_spam.csv\", 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, p_subject_word_given_spam_dict.keys(), lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        writer.writerow(p_subject_word_given_spam_dict)\n",
    "\n",
    "    with open(\"data/p_subject_word_given_ham.csv\", 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, p_subject_word_given_ham_dict.keys(), lineterminator='\\n')\n",
    "        writer.writeheader()\n",
    "        writer.writerow(p_subject_word_given_ham_dict)\n",
    "\n",
    "    print(\"...done! Csv-files saved to 'data/p_subject_word_given_spam.csv' and 'data/p_subject_word_given_ham.csv'\")\n",
    "# if word-specific probabilities have already been save to file, read them in and store in dictonary\n",
    "else:\n",
    "    print(\"Files with probabilities for subject line words found. Reading in probabilities to dictonaries...\", end = \"\")\n",
    "    with open(\"data/p_subject_word_given_spam.csv\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        p_subject_word_given_spam_dict = dict(next(reader))\n",
    "    \n",
    "    with open(\"data/p_subject_word_given_ham.csv\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        p_subject_word_given_ham_dict = dict(next(reader))\n",
    "    print(\"...Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-universal",
   "metadata": {},
   "source": [
    "### Build Spam Classification Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-account",
   "metadata": {},
   "source": [
    "The following function takes a subject line (as a string) and classifies the subject line as ham or spam based on the unnormalized posterior, computed  from the parameters derived from the training dataset. I do not normalize the posterior, because normalizing does not matter for the classification so we can avoid an extra computational step. By default the function returns a pandas series with a string classifying the message (\"ham\"/\"spam\"/\"unsure\") and the unnormalized posterior for ham and spam.\n",
    "The function also includes a test_mode (if test_mode = True). In test mode the function does not have return values, but instead prints the unnormalized posterior and the classification to the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proved-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_spam_subject(subject, test_mode = False):\n",
    "    '''\n",
    "    Takes a string subject line and classifies it as spam or ham\n",
    "    \n",
    "    test_mode = False returns a pandas series with a string classifying the message (\"ham\"/\"spam\"/\"unsure\")\n",
    "                and the unnormalized posterior for ham and spam\n",
    "    test_mode = True prints out a message to terminal with probabilities and classification\n",
    "    '''\n",
    "\n",
    "    # Remove all punctuation and convert everything to lowercase\n",
    "    message = re.sub('\\W', ' ', subject).lower()\n",
    "    # The method above is quick but produces double spaces - cleaning this up\n",
    "    message = re.sub('\\s{2,}', ' ', subject).strip()\n",
    "\n",
    "    # Convert message to list\n",
    "    word_list = subject.split(\" \")\n",
    "    \n",
    "    # intialize posteriors\n",
    "    p_spam_given_subject = np.float64(p_spam)\n",
    "    p_ham_given_subject = np.float64(p_ham)\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in p_subject_word_given_spam_dict:\n",
    "            # (approx. 2x faster than \"in voc\", but p_word_given_spam_dict and p_word_given_ham_dict have the same keys)\n",
    "            p_spam_given_subject *= np.float64(p_subject_word_given_spam_dict[word])\n",
    "            p_ham_given_subject *= np.float64(p_subject_word_given_ham_dict[word])\n",
    "    \n",
    "    # normalize posteriors\n",
    "    \n",
    "    if test_mode:\n",
    "        print('Unnormalized Posterior (Spam|message):', p_spam_given_subject)\n",
    "        print('Unnormalized Posterior (Ham|message):', p_ham_given_subject)\n",
    "\n",
    "        if p_ham_given_subject > p_spam_given_subject:\n",
    "            print('Label: Ham')\n",
    "        elif p_ham_given_subject < p_spam_given_subject:\n",
    "            print('Label: Spam')\n",
    "        else:\n",
    "            print('Equal proabilities, have a human classify this!')\n",
    "    else:\n",
    "        if p_ham_given_subject > p_spam_given_subject:\n",
    "            return pd.Series([\"ham\", p_ham_given_subject, p_spam_given_subject])\n",
    "        elif p_ham_given_subject < p_spam_given_subject:\n",
    "            return pd.Series([\"spam\", p_ham_given_subject, p_spam_given_subject])\n",
    "        else:\n",
    "            return pd.Series([\"unsure\", p_ham_given_subject, p_spam_given_subject])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-march",
   "metadata": {},
   "source": [
    "Below I will test this function with a random ham & spam message taken from my own e-mail account..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "owned-brave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify message:\n",
      "'Spatial Interpolation With and Without Predictor(s) plus 9 more'\n",
      "\n",
      "Unnormalized Posterior (Spam|message): 2.950055201297887e-18\n",
      "Unnormalized Posterior (Ham|message): 7.119251009175306e-12\n",
      "Label: Ham\n",
      "\n",
      "\n",
      "Classify message:\n",
      "'YOU HAVE SUCCESSFULY RECEIVED💲46.527,81 USD INTO YOUR ACCOUNT ⚠️CONFIRM BEFORE ⛔DELETE⛔ IN 48H⚠️'\n",
      "\n",
      "Unnormalized Posterior (Spam|message): 0.5085085085085085\n",
      "Unnormalized Posterior (Ham|message): 0.4914914914914915\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "# Test the Spam function\n",
    "print(\"Classify message:\\n'Spatial Interpolation With and Without Predictor(s) plus 9 more'\\n\")\n",
    "classify_spam_subject(\"Spatial Interpolation With and Without Predictor(s) plus 9 more\", test_mode = True)\n",
    ", \n",
    "print(\"\\n\\nClassify message:\\n'YOU HAVE SUCCESSFULY RECEIVED💲46.527,81 USD INTO YOUR ACCOUNT ⚠️CONFIRM BEFORE ⛔DELETE⛔ IN 48H⚠️'\\n\")\n",
    "classify_spam_subject('YOU HAVE SUCCESSFULY RECEIVED💲46.527,81 USD INTO YOUR ACCOUNT ⚠️CONFIRM BEFORE ⛔DELETE⛔ IN 48H⚠️', test_mode = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-burner",
   "metadata": {},
   "source": [
    "### Testing Classification Function on Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extraordinary-teacher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>unnorm post(Ham)</th>\n",
       "      <th>unnorm post(Spam)</th>\n",
       "      <th>Correct Categorization</th>\n",
       "      <th>Ham/Spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fw : re ivanhoe e . s . d</td>\n",
       "      <td>fyi , kim .\\n- - - - - original message - - - ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-07-09</td>\n",
       "      <td>spam</td>\n",
       "      <td>1.782793e-22</td>\n",
       "      <td>9.676542e-14</td>\n",
       "      <td>True</td>\n",
       "      <td>1.842386e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>start date : 2 / 6 / 02 ; hourahead hour : 24 ;</td>\n",
       "      <td>start date : 2 / 6 / 02 ; hourahead hour : 24 ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-11-28</td>\n",
       "      <td>spam</td>\n",
       "      <td>3.664495e-27</td>\n",
       "      <td>1.572757e-18</td>\n",
       "      <td>True</td>\n",
       "      <td>2.329982e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>re : aiesec polska - eurolds 2000</td>\n",
       "      <td>drogi panie andrzeju ,\\nprosze powolac sie na ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>ham</td>\n",
       "      <td>1.448162e-21</td>\n",
       "      <td>6.830164e-26</td>\n",
       "      <td>True</td>\n",
       "      <td>2.120245e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reviewer approval</td>\n",
       "      <td>please note that your employees have suggested...</td>\n",
       "      <td>ham</td>\n",
       "      <td>2000-05-22</td>\n",
       "      <td>ham</td>\n",
       "      <td>7.493524e-10</td>\n",
       "      <td>1.641797e-12</td>\n",
       "      <td>True</td>\n",
       "      <td>4.564220e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start date : 2 / 6 / 02 ; hourahead hour : 24 ;</td>\n",
       "      <td>start date : 2 / 6 / 02 ; hourahead hour : 24 ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-12-26</td>\n",
       "      <td>spam</td>\n",
       "      <td>3.664495e-27</td>\n",
       "      <td>1.572757e-18</td>\n",
       "      <td>True</td>\n",
       "      <td>2.329982e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Subject  \\\n",
       "Message ID                                                    \n",
       "0                                 fw : re ivanhoe e . s . d   \n",
       "1           start date : 2 / 6 / 02 ; hourahead hour : 24 ;   \n",
       "2                         re : aiesec polska - eurolds 2000   \n",
       "3                                         reviewer approval   \n",
       "4           start date : 2 / 6 / 02 ; hourahead hour : 24 ;   \n",
       "\n",
       "                                                      Message Spam/Ham  \\\n",
       "Message ID                                                               \n",
       "0           fyi , kim .\\n- - - - - original message - - - ...     spam   \n",
       "1           start date : 2 / 6 / 02 ; hourahead hour : 24 ...     spam   \n",
       "2           drogi panie andrzeju ,\\nprosze powolac sie na ...      ham   \n",
       "3           please note that your employees have suggested...      ham   \n",
       "4           start date : 2 / 6 / 02 ; hourahead hour : 24 ...     spam   \n",
       "\n",
       "                  Date Predicted  unnorm post(Ham)  unnorm post(Spam)  \\\n",
       "Message ID                                                              \n",
       "0           2005-07-09      spam      1.782793e-22       9.676542e-14   \n",
       "1           2004-11-28      spam      3.664495e-27       1.572757e-18   \n",
       "2           2000-02-22       ham      1.448162e-21       6.830164e-26   \n",
       "3           2000-05-22       ham      7.493524e-10       1.641797e-12   \n",
       "4           2004-12-26      spam      3.664495e-27       1.572757e-18   \n",
       "\n",
       "            Correct Categorization      Ham/Spam  \n",
       "Message ID                                        \n",
       "0                             True  1.842386e-09  \n",
       "1                             True  2.329982e-09  \n",
       "2                             True  2.120245e+04  \n",
       "3                             True  4.564220e+02  \n",
       "4                             True  2.329982e-09  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in test data\n",
    "test = pd.read_csv(\"data/test_data.zip\",\n",
    "                    compression=\"zip\", index_col=\"Message ID\")\n",
    "\n",
    "# Use Algorithm to determine p(Ham), p(Spam) and predicted categorization\n",
    "test[[\"Predicted\", \"unnorm post(Ham)\", \"unnorm post(Spam)\"]] = test['Subject'].apply(classify_spam_subject)\n",
    "# Record correct/incorrect categorizations\n",
    "test[\"Correct Categorization\"] = (test[\"Predicted\"]==test[\"Spam/Ham\"])\n",
    "# Calculate p(Ham)/p(Ham) Ratio to determine edge cases\n",
    "test[\"Ham/Spam\"] = test[\"unnorm post(Ham)\"]/test[\"unnorm post(Spam)\"]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-fitting",
   "metadata": {},
   "source": [
    "## Evaluate Performance of Classification by Subject Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vertical-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Spam Classification\n",
      "(what percentage of all messages is classified correctly)\n",
      "97.98 %\n",
      "\n",
      "Precision of Spam Classification \n",
      "(what percentage of spam classifications is correct)\n",
      "96.27 %\n",
      "\n",
      "Sensitivity of Spam Classification \n",
      "(what percentage of actual spam do we classify)\n",
      "100.0 %\n",
      "\n",
      "F1 Score:\n",
      "0.9809767177739921\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "spam_correct = 0\n",
    "ham_correct = 0\n",
    "total_messages = test.shape[0]\n",
    "spam_messages = test[test[\"Spam/Ham\"]==\"spam\"].shape[0]\n",
    "predicted_spam = test[test[\"Predicted\"]==\"spam\"].shape[0]\n",
    "total_correct = test[test[\"Correct Categorization\"]==True].shape[0]\n",
    "spam_correct = test[(test[\"Spam/Ham\"]==\"spam\") & (test[\"Correct Categorization\"]==True)].shape[0]\n",
    "        \n",
    "accuracy = total_correct / total_messages\n",
    "precision = spam_correct / predicted_spam\n",
    "sensitivity = spam_correct / spam_messages\n",
    "\n",
    "print(\"Accuracy of Spam Classification\\n(what percentage of all messages is classified correctly)\")\n",
    "print(str(round(accuracy * 100, 2)) + \" %\")\n",
    "\n",
    "print(\"\\nPrecision of Spam Classification \\n(what percentage of spam classifications is correct)\")\n",
    "print(str(round(precision * 100, 2)) + \" %\")\n",
    "\n",
    "print(\"\\nSensitivity of Spam Classification \\n(what percentage of actual spam do we classify)\")\n",
    "print(str(round(sensitivity * 100, 2)) + \" %\")\n",
    "\n",
    "print(\"\\nF1 Score:\")\n",
    "print(2*(sensitivity * precision) / (sensitivity + precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-penguin",
   "metadata": {},
   "source": [
    "Even though the algorithm so far only looked at the subject line it detects ***all*** spam emails in the test dataset!\n",
    "The precision is also remarkably high with 96%. This means only 4% of \"ham\" messages are incorrectly classified as spam.\n",
    "This example clearly shows that a simple algorithm like naive Bayes can achieve very accurate results given enough data.\n",
    "\n",
    "Things to do to improve the algorithm and/or efficiency (might try these things at a later point):\n",
    "* Ignore words with low specificity (p(spam) around 0.5) to make algorithm run faster\n",
    "* Use p(Ham)/p(Spam) ratio to detect edge cases with unsure classification. In first test runs 0.1 < \\[p(Ham)/p(Spam) ratio\\] < 10 seems to work well to catch cases with unreliable classification. Edge cases could then be run through a more computational intensive algorithm analysing the email text body."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 598.4,
   "position": {
    "height": "620.4px",
    "left": "625px",
    "right": "20px",
    "top": "33px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
