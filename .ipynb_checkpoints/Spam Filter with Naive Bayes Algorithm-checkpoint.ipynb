{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "future-dispatch",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this project I will be building a simple (but hopefully effective) spam filter for E-Mails using a naive Bayes approach on the Enron-Spam dataset. The dataset contains 33.716 Emails marked as spam or as non-spam (\"ham\") messages. The original dataset was collected by V. Metsis, I. Androutsopoulos and G. Paliouras. and can be found [here](http://www2.aueb.gr/users/ion/data/enron-spam). I packaged all the data into a [single csv-file](https://github.com/MWiechmann/enron_spam_data), which I will be using for this project.\n",
    "\n",
    "Comment about data and generated data go here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-visitor",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-vector",
   "metadata": {},
   "source": [
    "Quick Notes on current data (make more readable later):\n",
    "* enron_spam_data.zip: data set with Message ID, Subject, Spam/Ham, Date - unmodified data from repo\n",
    "* train.zip: 80% of original data set for training the model - structure like enron spam data, but subject and message have been processed: converted to lowercase, removed punctuation and transformed to list objects (one entry per word)\n",
    "* test.zip: 20% of original data set - no further processing yet (subject & message as simple string)\n",
    "* subject_voc.zip: Subject Line Vocabulary - one column per unique word, one row per Subject Line with word count per word per subject line, Message ID as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abandoned-executive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['re', 'tenaska', 'iv']</td>\n",
       "      <td>['i', 'tried', 'calling', 'you', 'this', 'am',...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-02-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['neon']</td>\n",
       "      <td>['bammel', 'neon', 'groups', 'fall', '2001', '...</td>\n",
       "      <td>ham</td>\n",
       "      <td>2001-09-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['fw', 're', 'ivanhoe', 'e', 's', 'd']</td>\n",
       "      <td>['fyi', 'kim', 'original', 'message', 'from', ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['start', 'date', '2', '6', '02', 'hourahead',...</td>\n",
       "      <td>['start', 'date', '2', '6', '02', 'hourahead',...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['fw', 're', 'ivanhoe', 'e', 's', 'd']</td>\n",
       "      <td>['fyi', 'kim', 'original', 'message', 'from', ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2004-09-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Subject  \\\n",
       "Message ID                                                      \n",
       "0                                     ['re', 'tenaska', 'iv']   \n",
       "1                                                    ['neon']   \n",
       "2                      ['fw', 're', 'ivanhoe', 'e', 's', 'd']   \n",
       "3           ['start', 'date', '2', '6', '02', 'hourahead',...   \n",
       "4                      ['fw', 're', 'ivanhoe', 'e', 's', 'd']   \n",
       "\n",
       "                                                      Message Spam/Ham  \\\n",
       "Message ID                                                               \n",
       "0           ['i', 'tried', 'calling', 'you', 'this', 'am',...     spam   \n",
       "1           ['bammel', 'neon', 'groups', 'fall', '2001', '...      ham   \n",
       "2           ['fyi', 'kim', 'original', 'message', 'from', ...     spam   \n",
       "3           ['start', 'date', '2', '6', '02', 'hourahead',...     spam   \n",
       "4           ['fyi', 'kim', 'original', 'message', 'from', ...     spam   \n",
       "\n",
       "                  Date  \n",
       "Message ID              \n",
       "0           2004-02-25  \n",
       "1           2001-09-24  \n",
       "2           2004-12-17  \n",
       "3           2005-08-31  \n",
       "4           2004-09-07  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "train = pd.read_csv(\"data/train_data.zip\",\n",
    "                    compression=\"zip\", index_col=\"Message ID\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "internal-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count\n",
      "spam    13716\n",
      "ham     13257\n",
      "Name: Spam/Ham, dtype: int64\n",
      "\n",
      "Proportion in %\n",
      "spam    50.85\n",
      "ham     49.15\n",
      "Name: Spam/Ham, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Count\")\n",
    "print(train[\"Spam/Ham\"].value_counts(dropna=False))\n",
    "print(\"\\nProportion in %\")\n",
    "print(round(train[\"Spam/Ham\"].value_counts(normalize=True), 4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anonymous-syndicate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 6845 entries, bold to doctor\n",
      "dtypes: int64(6845)"
     ]
    }
   ],
   "source": [
    "# template for reading in dask data\n",
    "\n",
    "if not os.path.exists(\"data/subject_voc.csv\"):\n",
    "    shutil.unpack_archive(\"data/subject_voc.csv\", \"data/\")\n",
    "subject_voc = dd.read_csv(\"data/subject_voc.csv\").set_index(\"Message ID\")\n",
    "subject_voc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-negotiation",
   "metadata": {},
   "source": [
    "# Building the Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-income",
   "metadata": {},
   "source": [
    "## General Constant Parameters for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excessive-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability for Spam and Non-Spam (Ham)\n",
    "p_spam = train[\"Spam/Ham\"].value_counts(normalize=True)[\"spam\"]\n",
    "p_ham = train[\"Spam/Ham\"].value_counts(normalize=True)[\"ham\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-mason",
   "metadata": {},
   "source": [
    "## Spam Filter based on Subject Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-vatican",
   "metadata": {},
   "source": [
    "### Calculating Subject Line Specific Constant Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "established-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Counts for Spam/ham\n",
    "train_spam = train[train[\"Spam/Ham\"] == \"spam\"]\n",
    "train_ham = train[train[\"Spam/Ham\"] == \"ham\"]\n",
    "\n",
    "n_words_subject_spam = train_spam[\"Subject\"].apply(len).sum()\n",
    "n_words_subject_ham = train_ham[\"Subject\"].apply(len).sum()\n",
    "# (could also use train[mask_spam].iloc[:,2:].sum().sum() above, but takes approx 2x as long)\n",
    "\n",
    "# Unique word count for vocabulary\n",
    "n_words_subject_voc = subject_voc.shape[1]\n",
    "\n",
    "# Smoothin parameter\n",
    "alpha = 1\n",
    "\n",
    "# Delete train_spam and train_ham again to save memory\n",
    "del train_spam, train_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-charles",
   "metadata": {},
   "source": [
    "### Calculate Word-Specific Parameters (Subject Line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pleasant-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_voc_words = list(subject_voc.columns[:2500])\n",
    "\n",
    "# Add Spam/Ham category to subject_voc_df and seperate into spam/ham dataframes\n",
    "subject_voc_spam = subject_voc[subject_voc_words].copy()\n",
    "subject_voc_ham = subject_voc[subject_voc_words].copy()\n",
    "subject_voc_spam[\"Spam/Ham\"] = train[\"Spam/Ham\"]\n",
    "subject_voc_ham[\"Spam/Ham\"] = train[\"Spam/Ham\"]\n",
    "subject_voc_spam = subject_voc_spam[subject_voc_ham[\"Spam/Ham\"] == \"spam\"]\n",
    "subject_voc_ham = subject_voc_ham[subject_voc_ham[\"Spam/Ham\"] == \"ham\"]\n",
    "subject_voc_spam = subject_voc_spam.compute()\n",
    "subject_voc_ham = subject_voc_ham.compute()\n",
    "\n",
    "# Build dictionaries with word-specific probability given either spam or non-spam (ham)\n",
    "p_subject_word_given_spam_dict = {word: 0.0 for word in subject_voc_words}\n",
    "p_subject_word_given_ham_dict = {word: 0.0 for word in subject_voc_words}\n",
    "\n",
    "for word in subject_voc_words:\n",
    "    n_word_given_spam = subject_voc_spam[word].sum()\n",
    "    prob = (n_word_given_spam + alpha) / \\\n",
    "        (n_words_subject_spam + alpha * n_words_subject_voc)\n",
    "    p_subject_word_given_spam_dict[word] = prob\n",
    "\n",
    "    n_word_given_ham = subject_voc_ham[word].sum()\n",
    "    prob = (n_word_given_ham + alpha) / \\\n",
    "        (n_words_subject_ham + alpha * n_words_subject_voc)\n",
    "    p_subject_word_given_ham_dict[word] = prob\n",
    "\n",
    "with open('data/p_subject_word_given_spam.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, p_subject_word_given_spam_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(p_subject_word_given_spam_dict)\n",
    "\n",
    "with open('data/p_subject_word_given_ham.csv', 'w') as f:\n",
    "    writer = csv.DictWriter(f, p_subject_word_given_ham_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(p_subject_word_given_ham_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bright-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "t = range(2500, n_words_subject_voc, 2500)\n",
    "\n",
    "for val in t:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "charged-macintosh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating dict with probabilities given spam/ham for 6845 words...\n",
      "...dictonary created!          3...\n",
      "Now saving dictonaries with probabilities to file...\n",
      "...done! Csv-files saved to 'data/p_subject_word_given_spam.csv' and 'data/p_subject_word_given_ham.csv'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Doing the following computation from dask dataframes is extremly slow.\n",
    "Instead I will create a series of pandas dataframes from the main dask dataframe to perform the operation on\n",
    "the result will be stored on file just in case.\n",
    "For this dataset, pandas dataframes with 2500 columns seem to work without memory problems on my machine\n",
    "Therefore the following process will go through the bigger dask dataframe with smaller pandas dataframe, \n",
    "each containing 2500 columns.\n",
    "'''\n",
    "\n",
    "print(\"creating dict with probabilities given spam/ham for \" +\n",
    "      str(n_words_subject_voc) + \" words...\")\n",
    "\n",
    "# Build dictionaries with word-specific probability given either spam or non-spam (ham)\n",
    "p_subject_word_given_spam_dict = {word: 0.0 for word in subject_voc.columns}\n",
    "p_subject_word_given_ham_dict = {word: 0.0 for word in subject_voc.columns}\n",
    "\n",
    "# Determine slice endpoints to go through dask dataframe in 2500 word steps\n",
    "endpoints = list(range(2500, n_words_subject_voc, 2500))\n",
    "endpoints.append(n_words_subject_voc)\n",
    "\n",
    "step = 1\n",
    "\n",
    "for endpoint in endpoints:\n",
    "    print(\"...creating dictonary - step \" + str(step) + \"/\" +\n",
    "          str(len(endpoints)) + \"...\", end=\"\\r\")\n",
    "\n",
    "    subject_voc_words_step = list(subject_voc.columns[:endpoint])\n",
    "\n",
    "    # Limit subject vocabulary dataframe to the 2500 words in this step, \n",
    "    # Add Spam/Ham to dataframe\n",
    "    # Seperate dataframe into spam/ham dataframes\n",
    "    # Then transform from dask to pandas dataframe\n",
    "    subject_voc_spam = subject_voc[subject_voc_words_step].copy()\n",
    "    subject_voc_ham = subject_voc[subject_voc_words_step].copy()\n",
    "    subject_voc_spam[\"Spam/Ham\"] = train[\"Spam/Ham\"]\n",
    "    subject_voc_ham[\"Spam/Ham\"] = train[\"Spam/Ham\"]\n",
    "    subject_voc_spam = subject_voc_spam[subject_voc_ham[\"Spam/Ham\"] == \"spam\"]\n",
    "    subject_voc_ham = subject_voc_ham[subject_voc_ham[\"Spam/Ham\"] == \"ham\"]\n",
    "    subject_voc_spam = subject_voc_spam.compute()\n",
    "    subject_voc_ham = subject_voc_ham.compute()\n",
    "\n",
    "    for word in subject_voc_words_step:\n",
    "        n_word_given_spam = subject_voc_spam[word].sum()\n",
    "        prob = (n_word_given_spam + alpha) / \\\n",
    "            (n_words_subject_spam + alpha * n_words_subject_voc)\n",
    "        p_subject_word_given_spam_dict[word] = prob\n",
    "\n",
    "        n_word_given_ham = subject_voc_ham[word].sum()\n",
    "        prob = (n_word_given_ham + alpha) / \\\n",
    "            (n_words_subject_ham + alpha * n_words_subject_voc)\n",
    "        p_subject_word_given_ham_dict[word] = prob\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"...dictonary created!                     \")\n",
    "\n",
    "print(\"Now saving dictonaries with probabilities to file...\")\n",
    "\n",
    "with open(\"data/p_subject_word_given_spam.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, p_subject_word_given_spam_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(p_subject_word_given_spam_dict)\n",
    "\n",
    "with open(\"data/p_subject_word_given_ham.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, p_subject_word_given_ham_dict.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(p_subject_word_given_ham_dict)\n",
    "    \n",
    "print(\"...done! Csv-files saved to 'data/p_subject_word_given_spam.csv' and 'data/p_subject_word_given_ham.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 598.4,
   "position": {
    "height": "620.4px",
    "left": "715px",
    "right": "20px",
    "top": "36px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
